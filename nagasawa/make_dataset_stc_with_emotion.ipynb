{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "from MeCab import Tagger\n",
    "import ipadic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "tagger = Tagger(ipadic.MECAB_ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 原型のリストを取得\n",
    "# count = 0\n",
    "# with open(\"data_src/wiki40b_one_stc/wiki_40b_test_one_stc.txt\", \"r\") as f:\n",
    "#     genkei_list = []\n",
    "#     for line in f:\n",
    "#         # count += 1\n",
    "#         tagger_list = tagger.parse(line).split(\"\\n\")\n",
    "#         del tagger_list[-2:]\n",
    "#         tmp_genkei_list = []\n",
    "#         # print(line) # for debug\n",
    "#         for i in tagger_list:\n",
    "#             i = i.split(\"\\t\")\n",
    "#             i += i[1].split(\",\")\n",
    "#             del i[1]\n",
    "#             tmp_genkei_list.append(i[7])\n",
    "#         genkei_list.append(tmp_genkei_list)\n",
    "#         del tmp_genkei_list\n",
    "#         # if count == 50:\n",
    "#         #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(30):\n",
    "#     print(genkei_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 感性語の抽出\n",
    "with open('/workspace/dataset/data_src/w_senti_vocab.txt', 'r') as f:\n",
    "    emotion_word_list = []\n",
    "    for line in f:\n",
    "        tmp_list = line.split()\n",
    "        emotion_word_list.append(tmp_list[0])\n",
    "# print(emotion_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(genkei_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1585"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emotion_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセット内に出現する感性語の個数をカウント\n",
    "# from tqdm import tqdm\n",
    "# all_emotion_word_count = 0\n",
    "# for emotion_word in tqdm(emotion_word_list):\n",
    "#     for genkeis in genkei_list:\n",
    "#         all_emotion_word_count += genkeis.count(emotion_word)\n",
    "# print(all_emotion_word_count)\n",
    "# valデータで169045回感性語が出現→169045×ウィンドウサイズ　のデータセットを生成可能(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 感性語を含む文章だけをtokenベースで抜き出し\n",
    "# from my_module.tools import has_emotion_word\n",
    "# with open('data_src/wiki40b_one_stc/wiki_40b_test_one_stc.txt', 'r') as f_read:\n",
    "#     with open('data_src/wiki40b_with_emotion/wiki_40b_with_emotion.txt', 'w') as f_write:\n",
    "#         count = 0\n",
    "#         for line in f_read:\n",
    "#             tokens = tokenizer.tokenize(line)\n",
    "#             if has_emotion_word(tokens, emotion_word_list):\n",
    "#                 count = count + 1\n",
    "#                 f_write.write(line)\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128698\n"
     ]
    }
   ],
   "source": [
    "# 感性語を含む文章だけをMeCabの分かち書きベースで抜き出し\n",
    "from my_module.tools import has_emotion_word, get_genkei_list\n",
    "with open('/workspace/dataset/data_src/wiki40b_one_stc/wiki_40b_test_one_stc.txt', 'r') as f_read:\n",
    "    with open('/workspace/dataset/data_src/wiki40b_with_emotion/test/wakati/wiki_40b_test_with_emotion.txt', 'w') as f_write:\n",
    "        count = 0\n",
    "        for line in f_read:\n",
    "            genkei_list = get_genkei_list(line, tagger)\n",
    "            if has_emotion_word(genkei_list, emotion_word_list):\n",
    "                count = count + 1\n",
    "                f_write.write(line)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # val, trainについても、感性語を含む文章だけを抜き出し\n",
    "# with open('data_src/wiki40b_one_stc/wiki_40b_val_one_stc.txt', 'r') as f_read:\n",
    "#     with open('data_src/wiki40b_with_emotion/wiki_40b_val_with_emotion.txt', 'w') as f_write:\n",
    "#         count = 0\n",
    "#         for line in f_read:\n",
    "#             tokens = tokenizer.tokenize(line)\n",
    "#             if has_emotion_word(tokens, emotion_word_list):\n",
    "#                 count = count + 1\n",
    "#                 f_write.write(line)\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128126\n"
     ]
    }
   ],
   "source": [
    "# # val, trainについても、感性語を含む文章だけを抜き出し\n",
    "with open('/workspace/dataset/data_src/wiki40b_one_stc/wiki_40b_val_one_stc.txt', 'r') as f_read:\n",
    "    with open('/workspace/dataset/data_src/wiki40b_with_emotion/val/wakati/wiki_40b_val_with_emotion.txt', 'w') as f_write:\n",
    "        count = 0\n",
    "        for line in f_read:\n",
    "            genkei_list = get_genkei_list(line, tagger)\n",
    "            if has_emotion_word(genkei_list, emotion_word_list):\n",
    "                count = count + 1\n",
    "                f_write.write(line)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('data_src/wiki40b_one_stc/wiki_40b_train_one_stc.txt', 'r') as f_read:\n",
    "#     with open('data_src/wiki40b_with_emotion/wiki_40b_train_with_emotion.txt', 'w') as f_write:\n",
    "#         count = 0\n",
    "#         for line in f_read:\n",
    "#             tokens = tokenizer.tokenize(line)\n",
    "#             if has_emotion_word(tokens, emotion_word_list):\n",
    "#                 count = count + 1\n",
    "#                 f_write.write(line)\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2337909\n"
     ]
    }
   ],
   "source": [
    "with open('/workspace/dataset/data_src/wiki40b_one_stc/wiki_40b_train_one_stc.txt', 'r') as f_read:\n",
    "    with open('/workspace/dataset/data_src/wiki40b_with_emotion/train/wakati/wiki_40b_train_with_emotion.txt', 'w') as f_write:\n",
    "        count = 0\n",
    "        for line in f_read:\n",
    "            genkei_list = get_genkei_list(line, tagger)\n",
    "            if has_emotion_word(genkei_list, emotion_word_list):\n",
    "                count = count + 1\n",
    "                f_write.write(line)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
